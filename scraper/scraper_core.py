import asyncio
from playwright.async_api import async_playwright
import requests

from .utils import FileUtils
from .content_extractor import ContentExtractor
from .keyword import KEYWORDS


class Scraper:
    """Main scraper class handling both requests + Playwright."""

    def __init__(self, out_dir: str, min_line_length: int = 50):
        self.out_dir = out_dir
        self.extractor = ContentExtractor(KEYWORDS, min_line_length)

    def scrape_with_requests(self, url: str):
        try:
            resp = requests.get(url, timeout=20, headers={"User-Agent": "Mozilla/5.0"})
            resp.raise_for_status()
            html = resp.text
            text = self.extractor.extract_blocks(html)
            if len(text.split()) < 50:
                return None, None
            return html, text
        except Exception:
            return None, None

    async def scrape_with_playwright(self, url: str, pw):
        browser = await pw.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto(url, timeout=60000)
            await page.wait_for_selector("body", timeout=20000)
            html = await page.content()
            text = self.extractor.extract_blocks(html)
            return html, text
        except Exception:
            return None, None
        finally:
            await browser.close()

    async def process_url(self, url: str, pw):
        """Process a single URL with requests first, then Playwright fallback."""
        html, text = self.scrape_with_requests(url)

        if not html or not text:
            html, text = await self.scrape_with_playwright(url, pw)

        if html and text:
            html_path, txt_path = FileUtils.get_output_paths(url, self.out_dir)
            html_path.write_text(html, encoding="utf-8")
            txt_path.write_text(text, encoding="utf-8")
            print(f"[OK] {url} â†’ {html_path.name}, {txt_path.name}")
        else:
            print(f"[FAIL] Could not scrape {url}")

    async def scrape_all(self, urls, parallel: int):
        """Scrape all URLs in parallel with Playwright."""
        sem = asyncio.Semaphore(parallel)

        async with async_playwright() as pw:

            async def bound_process(u):
                async with sem:
                    await self.process_url(u.strip(), pw)

            tasks = [bound_process(u) for u in urls if u.strip()]
            await asyncio.gather(*tasks)
